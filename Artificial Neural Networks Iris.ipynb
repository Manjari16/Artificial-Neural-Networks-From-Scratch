{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "5188a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing deps\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from math import exp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "df28a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Convert the target names to numbers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "iris_target_encoded = label_encoder.fit_transform(iris.target)\n",
    "\n",
    "# Create a DataFrame containing the feature columns\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Add the encoded target column to the DataFrame\n",
    "iris_df['target'] = iris_target_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "69b30d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the minimum and maximum values for each column\n",
    "def minmax(dataset): \n",
    "    stats = dataset.describe().loc[['min', 'max']].values.tolist()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "403cfc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize(data, minmax_set):\n",
    "    df= data.copy()\n",
    "    for i in range(len(df.columns) - 1):\n",
    "        df.iloc[:, i] = (df.iloc[:, i] - minmax_set[0][i]) / (minmax_set[1][i] - minmax_set[0][i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "4afdc2a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(df, n_folds):\n",
    "    df_split = list()\n",
    "    df_copy = df.copy()\n",
    "    fold_size = int(len(df) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = df_copy.sample(n=fold_size, replace=False)\n",
    "        df_copy = df_copy.drop(fold.index)\n",
    "        df_split.append(fold)\n",
    "    return df_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "73e246f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_met(actual, predicted):\n",
    "    # Convert input lists to Pandas Series for easy comparison\n",
    "    actual_series = pd.Series(actual)\n",
    "    predicted_series = pd.Series(predicted)\n",
    "\n",
    "    # Count the number of correct predictions\n",
    "    correct = (actual_series == predicted_series).sum()\n",
    "\n",
    "    # Calculate and return the accuracy as a percentage\n",
    "    accuracy = (correct / len(actual)) * 100.0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "633cf4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "\n",
    "def run_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    # Define a function to calculate accuracy percentage\n",
    "\n",
    "    # Split the dataset into cross-validation folds\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "\n",
    "    # Initialize a list to store accuracy scores for each fold\n",
    "    scores = list()\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        # Extract the test set for this fold\n",
    "        test_set = folds[i]\n",
    "\n",
    "        # Combine all other folds to form the training set\n",
    "        train_folds = [f for j, f in enumerate(folds) if j != i]\n",
    "        train_set = pd.concat(train_folds)\n",
    "\n",
    "        # Extract the actual target values for the test set\n",
    "        actual = test_set['target'].tolist()\n",
    "\n",
    "        # Make a copy of the test set and remove the target column for predictions\n",
    "        test_set_copy = test_set.copy()\n",
    "        test_set_copy['target'] = None\n",
    "        \n",
    "        print(f\"For Fold {i+1}:\")\n",
    "\n",
    "        # Make predictions using the algorithm (function) with given arguments\n",
    "        predicted = algorithm(train_set, test_set_copy, *args)\n",
    "        \n",
    "        accuracy = accuracy_met(actual, predicted)\n",
    "        cm = confusion_matrix(actual, predicted)\n",
    "        \n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "        FP = cm.sum(axis=0) - np.diag(cm)\n",
    "        FN = cm.sum(axis=1) - np.diag(cm)\n",
    "        TP = np.diag(cm)\n",
    "        TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "        TPR = TP / (TP + FN)\n",
    "        TNR = TN / (TN + FP)\n",
    "        Precision = TP / (TP + FP)\n",
    "        Recall = TP / (TP + FN)\n",
    "        Acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "        Fscore = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        k = cohen_kappa_score(actual, predicted)\n",
    "\n",
    "        print(f\"False Positives: {FP}\")\n",
    "        print(f\"False Negatives: {FN}\")\n",
    "        print(f\"True Positives: {TP}\")\n",
    "        print(f\"True Negatives: {TN}\")\n",
    "        print(f\"Sensitivity (True Positive Rate): {TPR}\")\n",
    "        print(f\"Specificity (True Negative Rate): {TNR}\")\n",
    "        print(f\"Precision: {Precision}\")\n",
    "        print(f\"Recall: {Recall}\")\n",
    "        print(f\"Accuracy: {Acc}\")\n",
    "        print(f\"F1 Score: {Fscore}\")\n",
    "        print(f\"Cohen's Kappa: {k}\")\n",
    "\n",
    "        scores.append(accuracy)\n",
    "        print('\\n\\n')\n",
    "\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "ebbd98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    # Add bias weight to the inputs\n",
    "    inputs_with_bias = inputs + [weights[-1]]\n",
    "\n",
    "    # Convert the weights and inputs to Pandas Series for element-wise multiplication\n",
    "    weights_series = pd.Series(weights[:-1])  # Exclude the bias weight\n",
    "    inputs_series = pd.Series(inputs_with_bias)\n",
    "\n",
    "    # Calculate the activation using element-wise multiplication and sum\n",
    "    activation = (weights_series * inputs_series).sum()\n",
    "\n",
    "    return activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "83d56bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer neuron activation using sigmoid function\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "4037afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "a3eb18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "        return output * (1.0 - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "3cd6d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "        for i in reversed(range(len(network))):\n",
    "                layer = network[i]\n",
    "                errors = list()\n",
    "                if i != len(network)-1: #if not output layer\n",
    "                        for j in range(len(layer)):\n",
    "                                error = 0.0\n",
    "                                for neuron in network[i + 1]:\n",
    "                                        error += (neuron['weights'][j] * neuron['delta'])\n",
    "                                errors.append(error)\n",
    "                else: #for output layer\n",
    "                        for j in range(len(layer)):\n",
    "                                neuron = layer[j]\n",
    "                                errors.append(expected[j] - neuron['output'])\n",
    "                for j in range(len(layer)):\n",
    "                        neuron = layer[j]\n",
    "                        neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "3da2f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "        for i in range(len(network)):\n",
    "                inputs = row[:-1]                \n",
    "                if i != 0: #if not input layer\n",
    "                        inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "                for neuron in network[i]:\n",
    "                        for j in range(len(inputs)):\n",
    "                                temp = l_rate * neuron['delta'] * inputs[j] + mu * neuron['prev'][j]\n",
    "                                \n",
    "                                neuron['weights'][j] += temp\n",
    "                                neuron['prev'][j] = temp\n",
    "                        temp = l_rate * neuron['delta'] + mu * neuron['prev'][-1]\n",
    "                        neuron['weights'][-1] += temp\n",
    "                        neuron['prev'][-1] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "0673f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        for _, row in train.iterrows():\n",
    "            outputs = forward_propagate(network, row.iloc[:4].values.tolist())\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[int(row['target'])] = 1\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row.values.tolist(), l_rate)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "fc141f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "        network = list()\n",
    "        hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)], 'prev':[0 for i in range(n_inputs+1)]} for i in range(n_hidden)]        \n",
    "        network.append(hidden_layer)\n",
    "        output_layer = [{'weights':[random() for i in range(n_hidden + 1)],'prev':[0 for i in range(n_hidden+1)]} for i in range(n_outputs)]\n",
    "        network.append(output_layer)\n",
    "        return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "2d5183c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "        outputs = forward_propagate(network, row)\n",
    "        return outputs.index(max(outputs))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "9e1b9b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "        n_inputs = train.shape[1] - 1\n",
    "        n_outputs = len(set([value for value in train.target]))\n",
    "        network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "        train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "        predictions = list()\n",
    "        for _, row in test.iterrows():\n",
    "                prediction = predict(network, row.iloc[:4].values.tolist())\n",
    "                predictions.append(prediction)\n",
    "        return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "141a82b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Fold 1:\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  7  1]\n",
      " [ 0  2 10]]\n",
      "False Positives: [0 2 1]\n",
      "False Negatives: [0 1 2]\n",
      "True Positives: [10  7 10]\n",
      "True Negatives: [20 20 17]\n",
      "Sensitivity (True Positive Rate): [1.         0.875      0.83333333]\n",
      "Specificity (True Negative Rate): [1.         0.90909091 0.94444444]\n",
      "Precision: [1.         0.77777778 0.90909091]\n",
      "Recall: [1.         0.875      0.83333333]\n",
      "Accuracy: [1.  0.9 0.9]\n",
      "F1 Score: [1.         0.82352941 0.86956522]\n",
      "Cohen's Kappa: 0.848993288590604\n",
      "\n",
      "\n",
      "\n",
      "For Fold 2:\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  6  2]\n",
      " [ 0  1 11]]\n",
      "False Positives: [0 1 2]\n",
      "False Negatives: [0 2 1]\n",
      "True Positives: [10  6 11]\n",
      "True Negatives: [20 21 16]\n",
      "Sensitivity (True Positive Rate): [1.         0.75       0.91666667]\n",
      "Specificity (True Negative Rate): [1.         0.95454545 0.88888889]\n",
      "Precision: [1.         0.85714286 0.84615385]\n",
      "Recall: [1.         0.75       0.91666667]\n",
      "Accuracy: [1.  0.9 0.9]\n",
      "F1 Score: [1.   0.8  0.88]\n",
      "Cohen's Kappa: 0.8469387755102041\n",
      "\n",
      "\n",
      "\n",
      "For Fold 3:\n",
      "Confusion Matrix:\n",
      "[[11  0  0]\n",
      " [ 0 11  2]\n",
      " [ 0  0  6]]\n",
      "False Positives: [0 0 2]\n",
      "False Negatives: [0 2 0]\n",
      "True Positives: [11 11  6]\n",
      "True Negatives: [19 17 22]\n",
      "Sensitivity (True Positive Rate): [1.         0.84615385 1.        ]\n",
      "Specificity (True Negative Rate): [1.         1.         0.91666667]\n",
      "Precision: [1.   1.   0.75]\n",
      "Recall: [1.         0.84615385 1.        ]\n",
      "Accuracy: [1.         0.93333333 0.93333333]\n",
      "F1 Score: [1.         0.91666667 0.85714286]\n",
      "Cohen's Kappa: 0.8979591836734694\n",
      "\n",
      "\n",
      "\n",
      "For Fold 4:\n",
      "Confusion Matrix:\n",
      "[[ 8  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  1 11]]\n",
      "False Positives: [0 1 0]\n",
      "False Negatives: [0 0 1]\n",
      "True Positives: [ 8 10 11]\n",
      "True Negatives: [22 19 18]\n",
      "Sensitivity (True Positive Rate): [1.         1.         0.91666667]\n",
      "Specificity (True Negative Rate): [1.   0.95 1.  ]\n",
      "Precision: [1.         0.90909091 1.        ]\n",
      "Recall: [1.         1.         0.91666667]\n",
      "Accuracy: [1.         0.96666667 0.96666667]\n",
      "F1 Score: [1.         0.95238095 0.95652174]\n",
      "Cohen's Kappa: 0.9494949494949495\n",
      "\n",
      "\n",
      "\n",
      "For Fold 5:\n",
      "Confusion Matrix:\n",
      "[[11  0  0]\n",
      " [ 0 11  0]\n",
      " [ 0  0  8]]\n",
      "False Positives: [0 0 0]\n",
      "False Negatives: [0 0 0]\n",
      "True Positives: [11 11  8]\n",
      "True Negatives: [19 19 22]\n",
      "Sensitivity (True Positive Rate): [1. 1. 1.]\n",
      "Specificity (True Negative Rate): [1. 1. 1.]\n",
      "Precision: [1. 1. 1.]\n",
      "Recall: [1. 1. 1.]\n",
      "Accuracy: [1. 1. 1.]\n",
      "F1 Score: [1. 1. 1.]\n",
      "Cohen's Kappa: 1.0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "# normalize input variables\n",
    "minmax_set= minmax(iris_df)\n",
    "dataset= normalize(iris_df, minmax_set)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.1\n",
    "mu=0.001\n",
    "n_epoch = 150\n",
    "n_hidden = 5\n",
    "scores = run_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "84d9f953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [90.0, 90.0, 93.33333333333333, 96.66666666666667, 100.0]\n",
      "average score: 94.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"scores: {scores}\")\n",
    "print(f\"average score: {sum(scores)/len(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
